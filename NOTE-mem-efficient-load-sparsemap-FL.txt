mem-efficient-load
sparsemap tensor exp notes

nov 9 2024

branch: mem-efficient-load

the code is at rwkv/sparse_exp/sparsemap.py 
    this is the main implementation. b/c tensor is difficult to wrap, so I ust write a bunch of functions, e.g. 
    to create a sparasemap tensor out of a disk file, or to convert a dense tensor to a disk file
    also "remap" --- maks/unmask some rows 

then I added exp code to rwkv/model.py and run.py (for pipeline)
deleting unused weights (e.g. CLS) and trnasofmr some dense tensors to sparsaemap 

mostly did exp with 0.1B

HIGH LEVEL OBS: 
pytorch itself takes 200MB (likely diffciult to save) 
tokenizer takes 300MB (!) and is used for both encodign and decoding (so cannot easily free it)
    for tiny model, something needs to be done with it. Otherwise, saving is minor.

delete unused tensors (then gc.collect()) can save memory. but has to be careful with linginering refernecs. otherwise no saving

THOUGHTS ON sparasemap tensors 

-it's functioning. to free certain rows, no need to unmap. just map that row (precisly, the residing page) to anon mkapping, kernel will 
free thie phsy memory immediately (gratt) 

    exp code is under sparse-exp/test-XXX

MJAOAR inefficeincy

- see above. tokenizer, pytorch. -- 600MB

about sparsemap tensor
- efficienct bottleneck 1. mapping must be done in page granulaty. therefmore the "page" sparsity is lower than the row sparsity. bad
    perhaps colocate the rows (seems the only way) 

- bottleneck 2. ffn.value is col baed. likely the activation map (the saprse one) must be transposed before muliplying with ffn.value
    therefore, it's more like the the ggml tensor format (always transposed)





